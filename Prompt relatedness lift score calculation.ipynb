{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores and average scores have been calculated and saved to final_cleaned_prompts_with_scores_and_average.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the final cleaned prompts and word score files\n",
    "final_cleaned_prompts_path = 'final_cleaned_prompts.csv'  # Adjust path if needed\n",
    "word_score_path = 'Word Score.csv'  # Adjust path if needed\n",
    "\n",
    "# Read data from CSV files\n",
    "final_cleaned_prompts_df = pd.read_csv(final_cleaned_prompts_path)\n",
    "word_score_df = pd.read_csv(word_score_path)\n",
    "\n",
    "# Convert the word score DataFrame to a dictionary for fast lookup\n",
    "word_score_dict = dict(zip(word_score_df[\"word\"], word_score_df[\"frequency\"]))\n",
    "\n",
    "# Function to calculate score and average score for each prompt\n",
    "def calculate_prompt_score_and_average(prompt, word_score_dict):\n",
    "    words = re.findall(r'\\b\\w+\\b', str(prompt).lower())  # Tokenize and convert to lowercase\n",
    "    score = sum(word_score_dict.get(word, 0) for word in words)  # Sum scores for each word in the prompt\n",
    "    length = len(words)  # Count words in the prompt\n",
    "    average_score = score / length if length > 0 else 0  # Calculate average score\n",
    "    return score, average_score\n",
    "\n",
    "# Apply the function to calculate scores and average scores for each prompt\n",
    "final_cleaned_prompts_df[[\"score\", \"average_score\"]] = final_cleaned_prompts_df[\"cleaned_prompt\"].apply(\n",
    "    lambda x: pd.Series(calculate_prompt_score_and_average(x, word_score_dict))\n",
    ")\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_path = 'final_cleaned_prompts_with_scores_and_average.csv'\n",
    "final_cleaned_prompts_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Scores and average scores have been calculated and saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightshade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
